# This values.yaml file creates the resources for random

multinode: false          # If true, creates LWS instead of deployments
inferencePool: true
inferenceModel: true
httpRoute: true

routing:
  # This is the model name for the OpenAI request
  modelName: facebook/opt-125m
  servicePort: 8000   # Sidecar listens on this port for requests. If there's no sidecar, the request goes here
  proxy:
    image: ghcr.io/llm-d/llm-d-routing-sidecar:0.0.6
    targetPort: 8000
  parentRefs:
  - group: gateway.networking.k8s.io
    kind: Gateway
    name: inference-gateway-2

modelArtifacts:
  uri: "pvc://vllm-llama-storage-claim/hub/models--TroyDoesAI--Llama-3.1-8B-Instruct/snapshots/3c168bd163fe098ebf4f7f7cbc731c53fee91d5a"

# describe decode pods
decode:
  replicas: 1
  containers:
  - name: "rank0"
    image: "quay.io/vllm/automation-vllm:llm-d-njhill-ext-lb-dp"
    imagePullPolicy: IfNotPresent
    # command:
    #   - vllm
    #   - serve
    args:
      - "--model"
      - "/model-cache/hub/models--TroyDoesAI--Llama-3.1-8B-Instruct/snapshots/3c168bd163fe098ebf4f7f7cbc731c53fee91d5a"
      - "--served-model-name"
      - "facebook/opt-125m"
      - --data-parallel-size
      - "2"
      - --data-parallel-rank
      - "0"
      - --data-parallel-address
      - "localhost"
      - --data-parallel-rpc-port
      - "13345"
      - "--port"
      - "8002"
    env:
      - name: "CUDA_VISIBLE_DEVICES"
        value: "0"
    ports:
      - containerPort: 8002
        protocol: TCP
    resources:
      limits:
        nvidia.com/gpu: 2
    mountModelVolume: true
  - name: "rank1"
    image: "quay.io/vllm/automation-vllm:llm-d-njhill-ext-lb-dp"
    imagePullPolicy: IfNotPresent
    # command:
    #   - vllm
    #   - serve
    args:
      - "--model"
      - "/model-cache/hub/models--TroyDoesAI--Llama-3.1-8B-Instruct/snapshots/3c168bd163fe098ebf4f7f7cbc731c53fee91d5a"
      - "--served-model-name"
      - "facebook/opt-125m"
      - --data-parallel-size
      - "2"
      - --data-parallel-rank
      - "1"
      - --data-parallel-address
      - "localhost"
      - --data-parallel-rpc-port
      - "13345"
      - "--port"
      - "8001"
    env:
      - name: "CUDA_VISIBLE_DEVICES"
        value: "1"
    ports:
      - containerPort: 8001
        protocol: TCP
    resources:
      limits:
        nvidia.com/gpu: 2
    mountModelVolume: true

endpointPicker:
  # This is for setting up a service more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/
  service:
    # This sets the service type more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types
    type: ClusterIP
    # This sets the ports more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/#field-spec-ports
    port: 9002
    targetPort: 9002
    appProtocol: http2
  debugLevel: 6
  disableReadinessProbe: true
  disableLivenessProbe: true
  replicas: 1
  permissions: pod-read