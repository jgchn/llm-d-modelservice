# vllm-sim values

# This values.yaml file creates the resources for random

multinode: false          # If true, creates LWS instead of deployments
inferencePool: false
httpRoute: true

routing:
  # This is the model name for the OpenAI request
  modelName: random
  servicePort: 8000   # Sidecar listens on this port for requests. If there's no sidecar, the request goes here
  proxy:
    image: ghcr.io/llm-d/llm-d-routing-sidecar:0.0.6
    targetPort: 8200
  parentRefs:
  - group: gateway.networking.k8s.io
    kind: Gateway
    name: llm-d-inference-gateway

modelArtifacts:
  uri: "hf://random/modelid"
  size: 5Mi

# describe decode pods
decode:
  replicas: 1
  containers:
  - name: "vllm"
    image: "ghcr.io/llm-d/llm-d-inference-sim:0.0.4"
    args:
      - "--model"
      - "random"
      - "--port"
      - "8200"  # targetPort
    ports:
      - containerPort: 5557
        protocol: TCP
    mountModelVolume: true
prefill:
  replicas: 1
  containers:
  - name: "vllm"
    image: "ghcr.io/llm-d/llm-d-inference-sim:0.0.4"
    args:
      - "--model"
      - "random"
      - "--port"
      - "8000"  # servicePort
    ports:
      - containerPort: 5557
        protocol: TCP
    mountModelVolume: true

endpointPicker:
  # This is for setting up a service more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/
  service:
    # This sets the service type more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types
    type: ClusterIP
    # This sets the ports more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/#field-spec-ports
    port: 9002
    targetPort: 9002
    appProtocol: http2
  debugLevel: 6
  disableReadinessProbe: true
  disableLivenessProbe: true
  replicas: 1
  permissions: pod-read

inferencepool:
  inferenceExtension:
    image:
      name: llm-d-inference-scheduler
      hub: "ghcr.io/llm-d"
      tag: "0.0.3"
      pullPolicy: "Always"
    env:
      ENABLE_KVCACHE_AWARE_SCORER: "false"
      ENABLE_LOAD_AWARE_SCORER: "true"
      ENABLE_PREFIX_AWARE_SCORER: "true"
      ENABLE_SESSION_AWARE_SCORER: "false"
      KVCACHE_AWARE_SCORER_WEIGHT: "1"
      KVCACHE_INDEXER_REDIS_ADDR: "1"
      PD_ENABLED: "false"
      PD_PROMPT_LEN_THRESHOLD: "10"
      PREFILL_ENABLE_KVCACHE_AWARE_SCORER: "false"
      PREFILL_ENABLE_LOAD_AWARE_SCORER: "false"
      PREFILL_ENABLE_PREFIX_AWARE_SCORER: "false"
      PREFILL_ENABLE_SESSION_AWARE_SCORER: "false"
      PREFILL_KVCACHE_AWARE_SCORER_WEIGHT: "1"
      PREFILL_KVCACHE_INDEXER_REDIS_ADDR: "1"
      PREFILL_PREFIX_AWARE_SCORER_WEIGHT: "1"
      PREFILL_SESSION_AWARE_SCORER_WEIGHT: "1"
      PREFIX_AWARE_SCORER_WEIGHT: "2"
      SESSION_AWARE_SCORER_WEIGHT: "1"
  inferencePool:
    targetPortNumber: 8000
    modelServerType: vllm
    modelServers:
      matchLabels:
      # Should be unique for each model service?
        llmd.ai: epp
