---
# Source: llm-d-modelservice/templates/epp-sa.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: dp-rank-llm-d-modelservice-epp-sa
  labels:
    helm.sh/chart: llm-d-modelservice-0.0.5
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
---
# Source: llm-d-modelservice/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: dp-rank-llm-d-modelservice-sa
  labels:
    helm.sh/chart: llm-d-modelservice-0.0.5
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
---
# Source: llm-d-modelservice/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: dp-rank-llm-d-modelservice-epp-rolebinding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: pod-read
subjects:
- kind: ServiceAccount
  name: dp-rank-llm-d-modelservice-epp-sa
---
# Source: llm-d-modelservice/templates/epp-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: dp-rank-llm-d-modelservice-epp-service
  labels:
    helm.sh/chart: llm-d-modelservice-0.0.5
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 9002
      targetPort: 9002
      protocol: TCP
      appProtocol: http2
  selector:
    llm-d.ai/epp: dp-rank-llm-d-modelservice-epp
---
# Source: llm-d-modelservice/templates/decode-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dp-rank-llm-d-modelservice-decode
  labels:
    helm.sh/chart: llm-d-modelservice-0.0.5
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      llm-d.ai/inferenceServing: "true"
      llm-d.ai/model: dp-rank-llm-d-modelservice
      llm-d.ai/role: decode
  template:
    metadata:
      labels:
        llm-d.ai/inferenceServing: "true"
        llm-d.ai/model: dp-rank-llm-d-modelservice
        llm-d.ai/role: decode
    spec:
      initContainers:
        - name: routing-proxy
          args:
            - --port=8000
            - --vllm-port=8000
            - --connector=nixlv2
            - -v=5
          image: ghcr.io/llm-d/llm-d-routing-sidecar:0.0.6
          imagePullPolicy: Always
          ports:
            - containerPort: 8000
          resources: {}
          restartPolicy: Always
          securityContext:
            allowPrivilegeEscalation: false
            runAsNonRoot: true
    
      serviceAccountName: dp-rank-llm-d-modelservice-sa
      volumes:
        - name: model-storage
          persistentVolumeClaim:
            claimName: vllm-llama-storage-claim
            readOnly: true
      containers:
        - name: rank0
          image: quay.io/vllm/automation-vllm:llm-d-njhill-ext-lb-dp
          imagePullPolicy: IfNotPresent
          args:
          # - "facebook/opt-125m"
          # - --port
          # - "8000"
          - --model
          - /model-cache/hub/models--TroyDoesAI--Llama-3.1-8B-Instruct/snapshots/3c168bd163fe098ebf4f7f7cbc731c53fee91d5a
          - --served-model-name
          - facebook/opt-125m
          - --data-parallel-size
          - "2"
          - --data-parallel-rank
          - "0"
          - --data-parallel-address
          - localhost
          - --data-parallel-rpc-port
          - "13345"
          - --port
          - "8002"
          env:
          - name: CUDA_VISIBLE_DEVICES
            value: "0"
          - name: HF_HOME
            value: /model-cache
          ports:
          - containerPort: 8002
            protocol: TCP
          
          resources:
            limits:
              nvidia.com/gpu: 2
            requests:
              {}
          volumeMounts:
          - name: model-storage
            mountPath: /model-cache
        - name: rank1
          image: quay.io/vllm/automation-vllm:llm-d-njhill-ext-lb-dp
          imagePullPolicy: IfNotPresent
          args:
          # - "facebook/opt-125m"
          # - --port
          # - "8000"
          - --model
          - /model-cache/hub/models--TroyDoesAI--Llama-3.1-8B-Instruct/snapshots/3c168bd163fe098ebf4f7f7cbc731c53fee91d5a
          - --served-model-name
          - facebook/opt-125m
          - --data-parallel-size
          - "2"
          - --data-parallel-rank
          - "1"
          - --data-parallel-address
          - localhost
          - --data-parallel-rpc-port
          - "13345"
          - --port
          - "8001"
          env:
          - name: CUDA_VISIBLE_DEVICES
            value: "1"
          - name: HF_HOME
            value: /model-cache
          ports:
          - containerPort: 8001
            protocol: TCP
          
          resources:
            limits:
              nvidia.com/gpu: 2
            requests:
              {}
          volumeMounts:
          - name: model-storage
            mountPath: /model-cache
---
# Source: llm-d-modelservice/templates/epp-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dp-rank-llm-d-modelservice-epp
  labels:
    llm-d.ai/epp: dp-rank-llm-d-modelservice-epp
  namespace: e2e-solution
spec:
  replicas: 1
  selector:
    matchLabels:
      llm-d.ai/epp: dp-rank-llm-d-modelservice-epp
  template:
    metadata:
      labels:
        llm-d.ai/epp: dp-rank-llm-d-modelservice-epp
    spec:
      containers:
      - name: epp
        imagePullPolicy: Always
        image: ghcr.io/llm-d/llm-d-inference-scheduler:0.0.3
        args:
        - --poolName
        - dp-rank-llm-d-modelservice-inference-pool
        - --poolNamespace
        - e2e-solution
        - -v
        - "6"
        - --zap-encoder
        - json
        - --grpcPort
        - "9002"
        - --grpcHealthPort
        - "9003"
        # using defaults from https://github.com/llm-d/llm-d-deployer/blob/main/charts/llm-d/values.yaml#L563-L603
        env:
        - name: ENABLE_KVCACHE_AWARE_SCORER
          value: "false"
        - name: ENABLE_LOAD_AWARE_SCORER
          value: "true"
        - name: ENABLE_PREFIX_AWARE_SCORER
          value: "true"
        - name: ENABLE_SESSION_AWARE_SCORER
          value: "false"
        - name: KVCACHE_AWARE_SCORER_WEIGHT
          value: "1"
        - name: KVCACHE_INDEXER_REDIS_ADDR
        - name: LOAD_AWARE_SCORER_WEIGHT
          value: "1"
        - name: PD_ENABLED
          value: "false"
        - name: PD_PROMPT_LEN_THRESHOLD
          value: "10"
        - name: PREFILL_ENABLE_KVCACHE_AWARE_SCORER
          value: "false"
        - name: PREFILL_ENABLE_LOAD_AWARE_SCORER
          value: "false"
        - name: PREFILL_ENABLE_PREFIX_AWARE_SCORER
          value: "false"
        - name: PREFILL_ENABLE_SESSION_AWARE_SCORER
          value: "false"
        - name: PREFILL_KVCACHE_AWARE_SCORER_WEIGHT
          value: "1"
        - name: PREFILL_KVCACHE_INDEXER_REDIS_ADDR
        - name: PREFILL_LOAD_AWARE_SCORER_WEIGHT
          value: "1"
        - name: PREFILL_PREFIX_AWARE_SCORER_WEIGHT
          value: "1"
        - name: PREFILL_SESSION_AWARE_SCORER_WEIGHT
          value: "1"
        - name: PREFIX_AWARE_SCORER_WEIGHT
          value: "2"
        - name: SESSION_AWARE_SCORER_WEIGHT
          value: "1"
        ports:
        - containerPort: 9002
          name: grpc
          protocol: TCP
        - containerPort: 9003
          name: grpc-health
          protocol: TCP
        - containerPort: 9090
          name: metrics
          protocol: TCP
      serviceAccount: dp-rank-llm-d-modelservice-epp-sa
      serviceAccountName: dp-rank-llm-d-modelservice-epp-sa
---
# Source: llm-d-modelservice/templates/prefill-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dp-rank-llm-d-modelservice-prefill
  labels:
    helm.sh/chart: llm-d-modelservice-0.0.5
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      llm-d.ai/inferenceServing: "true"
      llm-d.ai/model: dp-rank-llm-d-modelservice
      llm-d.ai/role: prefill
  template:
    metadata:
      labels:
        llm-d.ai/inferenceServing: "true"
        llm-d.ai/model: dp-rank-llm-d-modelservice
        llm-d.ai/role: prefill
    spec:
    
      serviceAccountName: dp-rank-llm-d-modelservice-sa
      volumes:
        - name: model-storage
          persistentVolumeClaim:
            claimName: vllm-llama-storage-claim
            readOnly: true
      containers:
        - name: vllm
          image: ghcr.io/llm-d/llm-d-inference-sim:0.0.4
          args:
          # - "facebook/opt-125m"
          # - --port
          # - "8000"
          - --model
          - random
          - --port
          - "8000"
          env:
          - name: HF_HOME
            value: /model-cache
          ports:
          - containerPort: 5557
            protocol: TCP
          
          resources:
            limits:
              {}
            requests:
              {}
          volumeMounts:
          - name: model-storage
            mountPath: /model-cache
---
# Source: llm-d-modelservice/templates/routing.yaml
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: dp-rank-llm-d-modelservice-http-route
  namespace: e2e-solution
  labels:
    helm.sh/chart: llm-d-modelservice-0.0.5
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  parentRefs:
  - group: gateway.networking.k8s.io
    kind: Gateway
    name: inference-gateway-2
  rules:
  - backendRefs:
    - group: inference.networking.x-k8s.io
      kind: InferencePool
      name: dp-rank-llm-d-modelservice-inference-pool
      port: 8000
      weight: 1
    matches:
    - path:
        type: PathPrefix
        value: /
      headers:
      - name: x-model-name
        type: Exact
        value: facebook/opt-125m
---
# Source: llm-d-modelservice/templates/routing.yaml
apiVersion: inference.networking.x-k8s.io/v1alpha2
kind: InferenceModel
metadata:
  name: dp-rank-llm-d-modelservice-inference-model
  namespace: e2e-solution
  labels:
    llm-d.ai/inferenceServing: "true"
    llm-d.ai/model: dp-rank-llm-d-modelservice
spec:
  modelName: facebook/opt-125m
  poolRef:
    group: inference.networking.x-k8s.io
    kind: InferencePool
    name: dp-rank-llm-d-modelservice-inference-pool
---
# Source: llm-d-modelservice/templates/routing.yaml
apiVersion: inference.networking.x-k8s.io/v1alpha2
kind: InferencePool
metadata:
  name: dp-rank-llm-d-modelservice-inference-pool
  namespace: e2e-solution
spec:
  extensionRef:
    failureMode: FailClose
    group: ""
    kind: Service
    name: dp-rank-llm-d-modelservice-epp-service
  selector:
    llm-d.ai/inferenceServing: "true"
    llm-d.ai/model: dp-rank-llm-d-modelservice
  targetPortNumber: 8000
